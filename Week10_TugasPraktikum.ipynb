{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFYq629RfsPJo9l0+fEIE0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MACHINE LEARNING**\n",
        "\n",
        "# Tugas Praktikum 10 - Recurrent Neural Network (RNN)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mMfJ0ILz91Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masfudin Dwi Cahyo\n",
        "\n",
        "# 2141720010\n",
        "\n",
        "# TI-3D\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-09J3s5SA85Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TUGAS**"
      ],
      "metadata": {
        "id": "PIkVD0U-BLln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Library**"
      ],
      "metadata": {
        "id": "bMDay7BEuJVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pFiZRGCNtjJ4"
      },
      "outputs": [],
      "source": [
        "# Library Tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Data**"
      ],
      "metadata": {
        "id": "X_lFcNx2vBU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Shakespeare\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "mMnT0lvWvTGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8895e147-77e3-4bd2-8dae-02a362d57e47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "id": "xpquWgEAvfMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68682445-6924-4909-ed31-1fb6dc701ae8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "id": "2UqNNIIavggq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4a58d7-712c-4575-ae71-3d96d97f8577"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "TksW6ITMvgkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09d1894-c210-4f3e-d29d-17cbf1e6c020"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Olah Teks**"
      ],
      "metadata": {
        "id": "2-5Zfx1MvdFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorize Teks**\n",
        "\n",
        "Sebelum training, Anda perlu mengonversi string menjadi representasi numerik. tf.keras.layers.StringLookup dapat mengubah setiap karakter menjadi ID numerik. Caranya adalah teks akan dipecah menjadi token terlebih dahulu."
      ],
      "metadata": {
        "id": "2T_09SKxvoDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "p-d2nwUuvrzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d03dd8-04ff-4f31-f582-4870ab75db11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sekarang buat tf.keras.layers.StringLookup layer:"
      ],
      "metadata": {
        "id": "0Az4VlvVv2Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "fJknvR_PvlKv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "perintah diatas mengconvert token menjadi id"
      ],
      "metadata": {
        "id": "plDveRIqv4XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "id": "gv3Dseojv6Yc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21d194a-b087-4149-964a-91b153cc926f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karena tujuan tutorial ini adalah untuk menghasilkan teks, penting juga untuk membalikkan representasi ini. Untuk ini Anda dapat menggunakan kode tf.keras.layers.StringLookup(..., invert=True).\n",
        "Catatan: pada kode ini, daripada meneruskan kosakata asli yang dihasilkan dengan diurutkan(set(teks)) gunakan metode get_vocabulary() dari tf.keras.layers.StringLookup sehingga token [UNK] disetel dengan cara yang sama."
      ],
      "metadata": {
        "id": "b3V6h7v9v7yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "WQTpON7WwGgl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor:"
      ],
      "metadata": {
        "id": "s86CoA0BwKIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "id": "xJZZ0wgowMqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bfbdbf9-1fe0-40f0-9349-21a729439eb7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anda dapat menggunakan tf.strings.reduce_join untuk menggabungkan kembali karakter menjadi string."
      ],
      "metadata": {
        "id": "aHMx-yIqwOM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "5sv5EQZDwT9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ce09a1-2252-4731-ed27-d880870707d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "4prT_9w5wVNO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediksi**"
      ],
      "metadata": {
        "id": "avPGg3bMwWb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diberikan sebuah karakter, atau serangkaian karakter, karakter apa yang paling mungkin berikutnya? Ini adalah tugas yang harus Anda latih agar model dapat melakukannya. Masukan ke model akan berupa urutan karakter, dan Anda melatih model untuk memprediksi keluaran berupa karakter berikut pada setiap langkah waktu. Karena RNN mempertahankan keadaan internal yang bergantung pada elemen yang terlihat sebelumnya, mengingat semua karakter dihitung hingga saat ini, karakter apa selanjutnya?"
      ],
      "metadata": {
        "id": "mJb20BVBwZJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Membuat Training Set dan Target**"
      ],
      "metadata": {
        "id": "P4U6sTpixNXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya bagilah teks menjadi contoh sequence. Setiap masukan sequence akan berisi karakter seq_length dari teks. Untuk setiap masukan sequence, target prediksi berisi teks dengan panjang yang sama, hanya digeser satu karakter ke kanan. Jadi, bagi teks menjadi beberapa bagian seq_length+1. Misalnya, seq_length adalah 4 dan teks kita adalah \"Hello\". Urutan masukannya adalah \"Hell\", dan urutan targetnya adalah \"ello\". Untuk melakukan ini, pertama-tama gunakan fungsi tf.data.Dataset.from_tensor_slices untuk mengonversi vektor teks menjadi aliran indeks karakter."
      ],
      "metadata": {
        "id": "1wu5Q4GRxemp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "KoMU6wWoxhW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2eb00c-b774-41f6-f171-d02517301a96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "DRHyRgQcxiiN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "kW68lAzyxj5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81caded0-69a3-40de-d715-92341a66a892"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "zaAxGUsDxlDC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan."
      ],
      "metadata": {
        "id": "rcv6Ykb8xmej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "1d4E1ujqxqb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbcc3f32-b560-4ea0-9541-05c7897c6512"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string:"
      ],
      "metadata": {
        "id": "b5mdnc6e1HzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "id": "Mwlx9UJD1KLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca80ee6b-a120-4eca-c653-6ef2b2c6de7a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk pelatihan, Anda memerlukan kumpulan data pasangan (input, label). Dimana input dan label merupakan urutan. Pada setiap langkah waktu, inputnya adalah karakter saat ini dan labelnya adalah karakter berikutnya. Berikut adalah fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu:"
      ],
      "metadata": {
        "id": "v-nEhP2J1Ru5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "4YfMb7NG1Uct"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "id": "UUtJxX461V5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d8449c-10e8-4364-c129-c8ddecadd60b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "L_IgWHpG1XvC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "Q2tTX2ji1Zkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb06f6d6-123a-4d3b-f6b9-45c08314844c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Membuat Batch Training**"
      ],
      "metadata": {
        "id": "c-urHHXi1bWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anda menggunakan tf.data untuk membagi teks menjadi sequence yang dapat diatur. Namun sebelum memasukkan data ini ke dalam model, Anda perlu mengacak data dan mengemasnya ke dalam batch."
      ],
      "metadata": {
        "id": "2v0hALhf1gQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19mLhb-hs-tf",
        "outputId": "c464575a-f62d-49d7-c9ee-524392bc4a8a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Buat Model**"
      ],
      "metadata": {
        "id": "K_AI4ZmOFPJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagian ini mendefinisikan model sebagai subkelas keras.Model (untuk lebih detilnya, lihat Making new Layers and Models via subclassing (https://www.tensorflow.org/guide/keras/custom_layers_and_models) )."
      ],
      "metadata": {
        "id": "ZBYQEYHDtEB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model yang kita bangun memiliki 3 lapisan neural network :\n",
        "\n",
        "- tf.keras.layers.Embedding: Lapisan masukan. Tabel pencarian yang dapat dilatih yang akan memetakan setiap karakter-ID ke vektor dengan dimensi embedding_dim;\n",
        "\n",
        "- tf.keras.layers.GRU: lapisan RNN dengan ukuran unit=rnn_units (Anda juga dapat menggunakan lapisan LSTM di sini.)\n",
        "\n",
        "- tf.keras.layers.Dense: Lapisan keluaran, dengan keluaran vocab_size. Ini menghasilkan satu logit untuk setiap karakter dalam kosakata. Ini adalah log kemungkinan setiap karakter menurut model."
      ],
      "metadata": {
        "id": "xaoOf_NstrXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "FRpHRPZStyH5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "CxcSV7PwtzV-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "DSHqZzYpt02L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Uji Model**"
      ],
      "metadata": {
        "id": "2nZcmJkWGH04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gunakan tf.GradientTape (https://www.tensorflow.org/api_docs/python/tf/GradientTape) untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca eager execution guide (https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "\n",
        "Prosedurnya adalah \"\n",
        "\n",
        "1. Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
        "\n",
        "2. Hitung update dan terapkan pada model dengan optimizer"
      ],
      "metadata": {
        "id": "yzWZLTh2BXcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "ks0J1YRIBW1o"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode diatas menerapkan train_step method sesuai dengan  Keras' train_step conventions (https://www.tensorflow.org/guide/keras customizing_what_happens_in_fit). Ini opsional, tetapi memungkinkan Anda mengubah perilaku langkah pelatihan dan tetap menggunakan keras Model.compile (https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) and Model.fit (https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) methods."
      ],
      "metadata": {
        "id": "jixyld8qB7yF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_tT9YYMxwJwi"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "ABFfqJeqCNzN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpTfwHX_CPbq",
        "outputId": "dfe9047b-99c8-4809-9ae8-1e06a2474fe4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 945s 5s/step - loss: 2.7448\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a73b2809c00>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "HNVxommlMFls"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atau jika ingin lebih mengetahui dalamnya, kita bisa membuat custom training loop sendiri"
      ],
      "metadata": {
        "id": "NRHMyrtNCQ8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "# saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J-mzQX0CUmY",
        "outputId": "d7f6d7ce-9bce-4909-c456-ba85bdd2a820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2319\n",
            "Epoch 1 Batch 50 Loss 2.0655\n",
            "Epoch 1 Batch 100 Loss 1.9297\n",
            "Epoch 1 Batch 150 Loss 1.8972\n",
            "\n",
            "Epoch 1 Loss: 2.0113\n",
            "Time taken for 1 epoch 981.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8793\n",
            "Epoch 2 Batch 50 Loss 1.7862\n",
            "Epoch 2 Batch 100 Loss 1.7068\n",
            "Epoch 2 Batch 150 Loss 1.6686\n",
            "\n",
            "Epoch 2 Loss: 1.7336\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5869\n",
            "Epoch 3 Batch 50 Loss 1.5615\n",
            "Epoch 3 Batch 100 Loss 1.5663\n",
            "Epoch 3 Batch 150 Loss 1.4885\n",
            "\n",
            "Epoch 3 Loss: 1.5702\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.5066\n",
            "Epoch 4 Batch 50 Loss 1.4814\n",
            "Epoch 4 Batch 100 Loss 1.4574\n",
            "Epoch 4 Batch 150 Loss 1.4919\n"
          ]
        }
      ]
    }
  ]
}